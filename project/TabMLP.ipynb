{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TabMLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "wOEHAijrg0Sr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import (\n",
        "    Any,\n",
        "    Dict,\n",
        "    List,\n",
        "    Match,\n",
        "    Tuple,\n",
        "    Union,\n",
        "    Callable,\n",
        "    Iterable,\n",
        "    Iterator,\n",
        "    Optional,\n",
        "    Generator,\n",
        "    Collection,\n",
        ")\n",
        "\n",
        "from torch import Tensor"
      ],
      "metadata": {
        "id": "oB5ctUYGg_pX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8CR-9qqGLCs3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "allowed_activations = [\n",
        "    \"relu\",\n",
        "    \"leaky_relu\",\n",
        "    \"softplus\",\n",
        "]\n",
        "\n",
        "\n",
        "def get_activation(activation):\n",
        "    if activation == \"relu\":\n",
        "        return nn.ReLU(inplace=True)\n",
        "    if activation == \"leaky_relu\":\n",
        "        return nn.LeakyReLU(inplace=True)\n",
        "    if activation == \"softplus\":\n",
        "        return nn.Softplus()\n",
        "\n",
        "\n",
        "def dense_layer(\n",
        "    inp: int,\n",
        "    out: int,\n",
        "    activation: str,\n",
        "    p: float,\n",
        "    bn: bool,\n",
        "    linear_first: bool,\n",
        "):\n",
        "    act_fn = get_activation(activation)\n",
        "    layers = [nn.BatchNorm1d(out if linear_first else inp)] if bn else []\n",
        "    if p != 0:\n",
        "        layers.append(nn.Dropout(p))  \n",
        "    lin = [nn.Linear(inp, out, bias=not bn), act_fn]  #batch norm has bias itself\n",
        "    layers = lin + layers if linear_first else layers + lin\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_hidden: List[int],\n",
        "        activation: str,\n",
        "        batchnorm: bool,\n",
        "        batchnorm_last: bool,\n",
        "        linear_first: bool,\n",
        "        dropout: List[float] = None,\n",
        "    ):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        if dropout is None:\n",
        "            dropout = [0.0] * len(d_hidden)\n",
        "\n",
        "        self.mlp = nn.Sequential()\n",
        "        for i in range(1, len(d_hidden)):\n",
        "            self.mlp.add_module(\n",
        "                \"dense_layer_{}\".format(i - 1),\n",
        "                dense_layer(\n",
        "                    d_hidden[i - 1],\n",
        "                    d_hidden[i],\n",
        "                    activation,\n",
        "                    dropout[i - 1],\n",
        "                    batchnorm and (i != len(d_hidden) - 1 or batchnorm_last),\n",
        "                    linear_first,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def forward(self, X: Tensor) -> Tensor:\n",
        "        return self.mlp(X)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vAnhd0IsOLxg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CatEmbeddingsAndCont(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        column_idx: Dict[str, int],\n",
        "        embed_input: List[Tuple[str, int, int]],\n",
        "        embed_dropout: float,\n",
        "        continuous_cols: Optional[List[str]],\n",
        "    ):\n",
        "        super(CatEmbeddingsAndCont, self).__init__()\n",
        "\n",
        "        self.column_idx = column_idx\n",
        "        self.embed_input = embed_input\n",
        "        self.continuous_cols = continuous_cols\n",
        "        # Categorical\n",
        "        if self.embed_input is not None:\n",
        "            self.embed_layers = nn.ModuleDict(\n",
        "                {\n",
        "                    \"emb_layer_\" + col: nn.Embedding(val + 1, dim, padding_idx=0) for col, val, dim in self.embed_input\n",
        "                }\n",
        "            )\n",
        "            self.embedding_dropout = nn.Dropout(embed_dropout)\n",
        "            self.emb_out_dim: int = int(\n",
        "                np.sum([embed[2] for embed in self.embed_input])\n",
        "            )\n",
        "        else:\n",
        "            self.emb_out_dim = 0\n",
        "\n",
        "        # Continuous\n",
        "        if self.continuous_cols is not None:\n",
        "            self.cont_idx = [self.column_idx[col] for col in self.continuous_cols]\n",
        "            self.cont_out_dim = len(self.continuous_cols)\n",
        "            self.cont_norm = nn.BatchNorm1d(self.cont_out_dim)\n",
        "        else:\n",
        "            self.cont_out_dim = 0\n",
        "\n",
        "        self.output_dim = self.emb_out_dim + self.cont_out_dim\n",
        "\n",
        "    def forward(self, X: Tensor) -> Tuple[Tensor, Any]:\n",
        "        if self.embed_input is not None:\n",
        "            embed = [\n",
        "                self.embed_layers[\"emb_layer_\" + col](X[:, self.column_idx[col]].long()) for col, _, _ in self.embed_input\n",
        "            ]\n",
        "            x_emb = torch.cat(embed, 1)\n",
        "            x_emb = self.embedding_dropout(x_emb)\n",
        "        else:\n",
        "            x_emb = None\n",
        "        if self.continuous_cols is not None:\n",
        "            x_cont = self.cont_norm((X[:, self.cont_idx].float()))\n",
        "        else:\n",
        "            x_cont = None\n",
        "\n",
        "        return x_emb, x_cont\n"
      ],
      "metadata": {
        "id": "Mo-p_AJ9LaWE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TabMlp(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        column_idx: Dict[str, int],\n",
        "        embed_input: Optional[List[Tuple[str, int, int]]] = None,\n",
        "        embed_dropout: float = 0.1,\n",
        "        continuous_cols: Optional[List[str]] = None,\n",
        "        cont_norm_layer: str = \"batchnorm\",\n",
        "        mlp_hidden_dims: List[int] = [200, 100],\n",
        "        mlp_activation: str = \"relu\",\n",
        "        mlp_dropout: List[float] = None,\n",
        "        mlp_batchnorm: bool = False,\n",
        "        mlp_batchnorm_last: bool = False,\n",
        "        mlp_linear_first: bool = False,\n",
        "        pred_dim: int = 1,\n",
        "    ):\n",
        "        super(TabMlp, self).__init__()\n",
        "\n",
        "        self.column_idx = column_idx\n",
        "        self.embed_input = embed_input\n",
        "        self.mlp_hidden_dims = mlp_hidden_dims\n",
        "        self.embed_dropout = embed_dropout\n",
        "        self.continuous_cols = continuous_cols\n",
        "        self.cont_norm_layer = cont_norm_layer\n",
        "        self.mlp_activation = mlp_activation\n",
        "        self.mlp_dropout = mlp_dropout\n",
        "        self.mlp_batchnorm = mlp_batchnorm\n",
        "        self.mlp_linear_first = mlp_linear_first\n",
        "        self.mlp_batchnorm_last = mlp_batchnorm_last\n",
        "        self.pred_dim = pred_dim\n",
        "\n",
        "        # CatEmbeddingsAndCont\n",
        "        self.cat_embed_and_cont = CatEmbeddingsAndCont(\n",
        "            column_idx,\n",
        "            embed_input,\n",
        "            embed_dropout,\n",
        "            continuous_cols,\n",
        "        )\n",
        "\n",
        "        # MLP\n",
        "        mlp_input_dim = self.cat_embed_and_cont.output_dim\n",
        "        mlp_hidden_dims = [mlp_input_dim] + mlp_hidden_dims\n",
        "        self.tab_mlp = MLP(\n",
        "            d_hidden=mlp_hidden_dims,\n",
        "            activation = self.mlp_activation,\n",
        "            dropout = self.mlp_dropout,\n",
        "            batchnorm = self.mlp_batchnorm,\n",
        "            batchnorm_last = self.mlp_batchnorm_last,\n",
        "            linear_first = self.mlp_linear_first,\n",
        "        )\n",
        "\n",
        "\n",
        "        # the output_dim attribute will be used as input_dim when \"merging\" the models\n",
        "        self.output_dim = mlp_hidden_dims[-1]\n",
        "\n",
        "        self.pred_layer = nn.Linear(self.output_dim, self.pred_dim)\n",
        "\n",
        "    def forward(self, X: Tensor) -> Tensor:\n",
        "        x_emb, x_cont = self.cat_embed_and_cont(X)\n",
        "        if x_emb is not None:\n",
        "            x = x_emb\n",
        "        if x_cont is not None:\n",
        "            x = torch.cat([x, x_cont], 1) if x_emb is not None else x_cont\n",
        "        return self.pred_layer(self.tab_mlp(x))"
      ],
      "metadata": {
        "id": "KKqZHH8iOFEa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/ahbagheri01/ML_Project.git\n",
        "%cd ML_Project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ-akcuBiPnq",
        "outputId": "d68433bf-c7ce-43d8-8779-25043e53605c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ML_Project' already exists and is not an empty directory.\n",
            "/content/ML_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/ML_Project/datasets/EDA.csv')\n",
        "data = data.iloc[: , 1:]\n",
        "df = data.drop(columns = [\"SalesAmountInEuro\",\"product_price\",\"time_delay_for_conversion\",\"click_timestamp\",\n",
        "                          \"day\",\"day_time\",\"user_id\",\"product_id\",\"product_title\"])"
      ],
      "metadata": {
        "id": "Yxfm9L1qezgT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = [\"tree_encode\",\"category_encode\", \"product_brand\",\n",
        "                        \"product_category(6)\", \"product_category(3)\",\"product_category(4)\",\"product_category(5)\"])\n",
        "categorial_col = [\"product_age_group\",\"device_type\",\"partner_id\", \"audience_id\",\n",
        "                  \"product_gender\",\"product_category(1)\",\"product_category(2)\",\n",
        "                 \"product_country\",\"day_time_category\"]\n",
        "numerical_col = [\"nb_clicks_1week\"]\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "df[numerical_col] = sc.fit_transform(df[numerical_col])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "aD_LLvsde3Lz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "d53dc278-8ea2-444d-ecd6-4962c4214a5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b3bd5cb8-4326-406d-8805-301b64ac1945\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sale</th>\n",
              "      <th>nb_clicks_1week</th>\n",
              "      <th>product_age_group</th>\n",
              "      <th>device_type</th>\n",
              "      <th>audience_id</th>\n",
              "      <th>product_gender</th>\n",
              "      <th>product_category(1)</th>\n",
              "      <th>product_category(2)</th>\n",
              "      <th>product_country</th>\n",
              "      <th>partner_id</th>\n",
              "      <th>day_time_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.792466</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.792466</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.792466</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.792466</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.792466</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3bd5cb8-4326-406d-8805-301b64ac1945')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3bd5cb8-4326-406d-8805-301b64ac1945 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3bd5cb8-4326-406d-8805-301b64ac1945');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Sale  nb_clicks_1week  ...  partner_id  day_time_category\n",
              "0   0.0         0.792466  ...           0                  4\n",
              "1   0.0         0.792466  ...           0                  1\n",
              "2   0.0         0.792466  ...           1                 16\n",
              "3   0.0         0.792466  ...           0                 20\n",
              "4   0.0         0.792466  ...           2                 20\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['day_time_category'] = (df['day_time_category']//6)"
      ],
      "metadata": {
        "id": "-PXEfP2WZhwu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_cols = []\n",
        "for c in categorial_col:\n",
        "  embed_cols.append((c,df[c].max()+1))\n",
        "print(embed_cols)\n",
        "cont_cols = [\"nb_clicks_1week\"]\n",
        "target = (df[\"Sale\"].values).astype(int)\n",
        "\n"
      ],
      "metadata": {
        "id": "IbEAS-qce6Tp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3287cb5b-4329-450c-8199-717ea39af7b9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('product_age_group', 9), ('device_type', 4), ('partner_id', 183), ('audience_id', 3182), ('product_gender', 11), ('product_category(1)', 22), ('product_category(2)', 145), ('product_country', 17), ('day_time_category', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.DataFrame()\n",
        "column_idx = {}\n",
        "idx = 0\n",
        "for c in categorial_col + numerical_col:\n",
        "  final_df[c] = df[c]\n",
        "  column_idx[c] = idx\n",
        "  idx += 1\n",
        "\n",
        "print(column_idx)"
      ],
      "metadata": {
        "id": "IaCmrigbfvQr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e3e1de-5785-4e7b-fd71-23bcaae930e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'product_age_group': 0, 'device_type': 1, 'partner_id': 2, 'audience_id': 3, 'product_gender': 4, 'product_category(1)': 5, 'product_category(2)': 6, 'product_country': 7, 'day_time_category': 8, 'nb_clicks_1week': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_input = []\n",
        "for i in range(len(embed_cols)):\n",
        "  val = embed_cols[i][1]\n",
        "  if embed_cols[i][1] > 100:\n",
        "    val = 100\n",
        "  embeddings_input.append((embed_cols[i][0],embed_cols[i][1],val))\n",
        "embeddings_input\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBwCO8HY4os5",
        "outputId": "e7f41b84-b0af-40e9-ba97-a5ee13dd96a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('product_age_group', 9, 9),\n",
              " ('device_type', 4, 4),\n",
              " ('partner_id', 183, 100),\n",
              " ('audience_id', 3182, 100),\n",
              " ('product_gender', 11, 11),\n",
              " ('product_category(1)', 22, 22),\n",
              " ('product_category(2)', 145, 100),\n",
              " ('product_country', 17, 17),\n",
              " ('day_time_category', 4, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tabmlp = TabMlp(\n",
        "    mlp_hidden_dims=[300, 200, 100],\n",
        "    column_idx=column_idx,\n",
        "    embed_input=embeddings_input,\n",
        "    mlp_dropout=[0.2,0.2,0.2],\n",
        "    continuous_cols=cont_cols,\n",
        "\n",
        "    mlp_batchnorm=True,\n",
        "    pred_dim = 2,\n",
        ")"
      ],
      "metadata": {
        "id": "1gh4motF4bfm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tabmlp"
      ],
      "metadata": {
        "id": "YHVtFQ7qBayW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "y = target\n",
        "X = final_df.to_numpy()\n",
        "\n",
        "X_train,X_test,y_train,y_test= train_test_split(X,\n",
        "                                                y,\n",
        "                                                test_size=0.25,\n",
        "                                                random_state=42,\n",
        "                                                shuffle=True)\n"
      ],
      "metadata": {
        "id": "mA5zyNNJBiOU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "count=Counter(y_train)\n",
        " \n",
        "class_count=np.array([count[0],count[1]])\n",
        " \n",
        "weight=1./class_count\n",
        "print(weight)\n",
        "samples_weight = np.array([weight[int(t)] for t in y_train])\n",
        "samples_weight = torch.from_numpy(samples_weight)\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, len(samples_weight))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAmqPypZCYtL",
        "outputId": "3e3cffd0-b516-40ef-fdee-42988c5e0527"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.54561894e-05 9.70779536e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset = torch.utils.data.TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
        "val_subset = torch.utils.data.TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))"
      ],
      "metadata": {
        "id": "gGn_w_hSCmDB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "NUMBER_OF_EPOCHS = 30\n",
        "train_loader = DataLoader(dataset=train_subset, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "val_loader = DataLoader(dataset=val_subset, shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuabX7ojA7VW",
        "outputId": "8cf39ee9-5135-487d-855b-3b02a02bcddf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "import tqdm\n",
        "\n",
        "all_train_losses = []\n",
        "all_train_accuracy = []\n",
        "all_val_losses = []\n",
        "all_val_accuracy = []\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    # training\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    acc_list = []\n",
        "    epoch_all = 0\n",
        "    model.train()\n",
        "    with tqdm.tqdm(enumerate(train_loader), total=len(train_loader)) as pbar:\n",
        "        for i, (X, y) in pbar:\n",
        "            X , y = X.to(device), y.to(device,dtype = torch.long)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = loss_function (outputs , y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += outputs.shape[0] * loss.item()\n",
        "            epoch_all += y.size(0)\n",
        "            predicted = torch.max(outputs.data,1)[1]\n",
        "            correct += (predicted == y).sum().item()\n",
        "            pbar.set_description(f'epoch {epoch } - train Loss: {epoch_loss / (i + 1):.3e} - train Acc: {correct * 100. / epoch_all:.2f}%')\n",
        "    loss_list = []\n",
        "    scheduler.step()\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        main = []\n",
        "        pre = []\n",
        "        epoch_loss = 0\n",
        "        epoch_all = 0\n",
        "        correct = 0\n",
        "        corr = 0\n",
        "        tot = 0\n",
        "        with tqdm.tqdm(enumerate(val_loader), total=len(val_loader)) as pbar:\n",
        "            for i, (X, y) in pbar:\n",
        "                X , y = X.to(device), y.to(device, dtype = torch.long)\n",
        "                out = model(X)\n",
        "                los = loss_function (out , y).item()\n",
        "                main.append(y)\n",
        "                epoch_loss += los\n",
        "                loss_list.append(los)\n",
        "                predicts = torch.max(out.data,1)[1]\n",
        "                pre.append(predicts)\n",
        "                epoch_all+=y.size(0)\n",
        "                tot += y.size(0)\n",
        "                correct += (predicts == y).sum().item()\n",
        "                pbar.set_description(f'epoch {epoch } - val Loss: {epoch_loss / (i + 1):.3e} - val Acc: {correct * 100. / epoch_all:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrUKwntTC-jq",
        "outputId": "bea3dbd0-2e9e-4218-d01b-6bea6b37d762"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 0 - train Loss: 7.839e+01 - train Acc: 65.51%: 100%|██████████| 586/586 [00:08<00:00, 72.57it/s]\n",
            "epoch 0 - val Loss: 5.336e-01 - val Acc: 70.35%: 100%|██████████| 196/196 [00:01<00:00, 127.98it/s]\n",
            "epoch 1 - train Loss: 7.416e+01 - train Acc: 68.70%: 100%|██████████| 586/586 [00:09<00:00, 64.50it/s]\n",
            "epoch 1 - val Loss: 5.913e-01 - val Acc: 61.71%: 100%|██████████| 196/196 [00:01<00:00, 163.49it/s]\n",
            "epoch 2 - train Loss: 7.254e+01 - train Acc: 69.20%: 100%|██████████| 586/586 [00:04<00:00, 120.17it/s]\n",
            "epoch 2 - val Loss: 5.954e-01 - val Acc: 64.42%: 100%|██████████| 196/196 [00:00<00:00, 199.15it/s]\n",
            "epoch 3 - train Loss: 7.172e+01 - train Acc: 69.78%: 100%|██████████| 586/586 [00:04<00:00, 121.12it/s]\n",
            "epoch 3 - val Loss: 5.833e-01 - val Acc: 63.03%: 100%|██████████| 196/196 [00:00<00:00, 196.31it/s]\n",
            "epoch 4 - train Loss: 7.117e+01 - train Acc: 70.04%: 100%|██████████| 586/586 [00:04<00:00, 119.97it/s]\n",
            "epoch 4 - val Loss: 6.118e-01 - val Acc: 66.13%: 100%|██████████| 196/196 [00:01<00:00, 194.55it/s]\n",
            "epoch 5 - train Loss: 7.067e+01 - train Acc: 70.25%: 100%|██████████| 586/586 [00:04<00:00, 120.23it/s]\n",
            "epoch 5 - val Loss: 5.894e-01 - val Acc: 64.22%: 100%|██████████| 196/196 [00:01<00:00, 188.22it/s]\n",
            "epoch 6 - train Loss: 7.019e+01 - train Acc: 70.38%: 100%|██████████| 586/586 [00:04<00:00, 120.03it/s]\n",
            "epoch 6 - val Loss: 6.118e-01 - val Acc: 62.14%: 100%|██████████| 196/196 [00:01<00:00, 194.59it/s]\n",
            "epoch 7 - train Loss: 6.985e+01 - train Acc: 70.49%: 100%|██████████| 586/586 [00:04<00:00, 120.14it/s]\n",
            "epoch 7 - val Loss: 6.175e-01 - val Acc: 65.55%: 100%|██████████| 196/196 [00:01<00:00, 191.79it/s]\n",
            "epoch 8 - train Loss: 6.943e+01 - train Acc: 70.93%: 100%|██████████| 586/586 [00:04<00:00, 118.90it/s]\n",
            "epoch 8 - val Loss: 6.628e-01 - val Acc: 64.09%: 100%|██████████| 196/196 [00:01<00:00, 192.86it/s]\n",
            "epoch 9 - train Loss: 6.944e+01 - train Acc: 70.49%: 100%|██████████| 586/586 [00:04<00:00, 119.75it/s]\n",
            "epoch 9 - val Loss: 6.121e-01 - val Acc: 64.91%: 100%|██████████| 196/196 [00:01<00:00, 190.67it/s]\n",
            "epoch 10 - train Loss: 6.822e+01 - train Acc: 70.93%: 100%|██████████| 586/586 [00:04<00:00, 117.73it/s]\n",
            "epoch 10 - val Loss: 6.293e-01 - val Acc: 65.16%: 100%|██████████| 196/196 [00:01<00:00, 180.30it/s]\n",
            "epoch 11 - train Loss: 6.771e+01 - train Acc: 71.43%: 100%|██████████| 586/586 [00:04<00:00, 117.33it/s]\n",
            "epoch 11 - val Loss: 6.291e-01 - val Acc: 64.93%: 100%|██████████| 196/196 [00:01<00:00, 191.82it/s]\n",
            "epoch 12 - train Loss: 6.735e+01 - train Acc: 71.60%: 100%|██████████| 586/586 [00:04<00:00, 118.01it/s]\n",
            "epoch 12 - val Loss: 6.359e-01 - val Acc: 65.16%: 100%|██████████| 196/196 [00:01<00:00, 190.84it/s]\n",
            "epoch 13 - train Loss: 6.729e+01 - train Acc: 71.54%: 100%|██████████| 586/586 [00:04<00:00, 118.52it/s]\n",
            "epoch 13 - val Loss: 6.417e-01 - val Acc: 65.37%: 100%|██████████| 196/196 [00:01<00:00, 189.35it/s]\n",
            "epoch 14 - train Loss: 6.699e+01 - train Acc: 71.74%: 100%|██████████| 586/586 [00:04<00:00, 118.31it/s]\n",
            "epoch 14 - val Loss: 6.209e-01 - val Acc: 65.63%: 100%|██████████| 196/196 [00:01<00:00, 192.59it/s]\n",
            "epoch 15 - train Loss: 6.720e+01 - train Acc: 71.38%: 100%|██████████| 586/586 [00:04<00:00, 117.48it/s]\n",
            "epoch 15 - val Loss: 6.350e-01 - val Acc: 65.79%: 100%|██████████| 196/196 [00:01<00:00, 192.03it/s]\n",
            "epoch 16 - train Loss: 6.669e+01 - train Acc: 71.77%: 100%|██████████| 586/586 [00:04<00:00, 117.43it/s]\n",
            "epoch 16 - val Loss: 6.170e-01 - val Acc: 64.61%: 100%|██████████| 196/196 [00:01<00:00, 187.15it/s]\n",
            "epoch 17 - train Loss: 6.669e+01 - train Acc: 71.54%: 100%|██████████| 586/586 [00:04<00:00, 117.70it/s]\n",
            "epoch 17 - val Loss: 6.149e-01 - val Acc: 66.48%: 100%|██████████| 196/196 [00:01<00:00, 189.22it/s]\n",
            "epoch 18 - train Loss: 6.667e+01 - train Acc: 71.50%: 100%|██████████| 586/586 [00:05<00:00, 116.72it/s]\n",
            "epoch 18 - val Loss: 6.288e-01 - val Acc: 66.44%: 100%|██████████| 196/196 [00:01<00:00, 177.41it/s]\n",
            "epoch 19 - train Loss: 6.667e+01 - train Acc: 71.76%: 100%|██████████| 586/586 [00:05<00:00, 108.92it/s]\n",
            "epoch 19 - val Loss: 6.333e-01 - val Acc: 64.97%: 100%|██████████| 196/196 [00:01<00:00, 182.08it/s]\n",
            "epoch 20 - train Loss: 6.624e+01 - train Acc: 71.93%: 100%|██████████| 586/586 [00:05<00:00, 114.45it/s]\n",
            "epoch 20 - val Loss: 6.521e-01 - val Acc: 64.18%: 100%|██████████| 196/196 [00:01<00:00, 185.76it/s]\n",
            "epoch 21 - train Loss: 6.630e+01 - train Acc: 71.84%: 100%|██████████| 586/586 [00:05<00:00, 113.49it/s]\n",
            "epoch 21 - val Loss: 6.408e-01 - val Acc: 64.76%: 100%|██████████| 196/196 [00:01<00:00, 186.41it/s]\n",
            "epoch 22 - train Loss: 6.639e+01 - train Acc: 71.67%: 100%|██████████| 586/586 [00:04<00:00, 117.70it/s]\n",
            "epoch 22 - val Loss: 6.447e-01 - val Acc: 65.00%: 100%|██████████| 196/196 [00:01<00:00, 187.85it/s]\n",
            "epoch 23 - train Loss: 6.619e+01 - train Acc: 71.96%: 100%|██████████| 586/586 [00:05<00:00, 115.91it/s]\n",
            "epoch 23 - val Loss: 6.459e-01 - val Acc: 65.07%: 100%|██████████| 196/196 [00:01<00:00, 188.89it/s]\n",
            "epoch 24 - train Loss: 6.607e+01 - train Acc: 72.00%: 100%|██████████| 586/586 [00:05<00:00, 116.97it/s]\n",
            "epoch 24 - val Loss: 6.374e-01 - val Acc: 65.60%: 100%|██████████| 196/196 [00:01<00:00, 189.61it/s]\n",
            "epoch 25 - train Loss: 6.616e+01 - train Acc: 71.81%: 100%|██████████| 586/586 [00:05<00:00, 115.60it/s]\n",
            "epoch 25 - val Loss: 6.411e-01 - val Acc: 64.97%: 100%|██████████| 196/196 [00:01<00:00, 186.34it/s]\n",
            "epoch 26 - train Loss: 6.573e+01 - train Acc: 72.25%: 100%|██████████| 586/586 [00:05<00:00, 113.72it/s]\n",
            "epoch 26 - val Loss: 6.511e-01 - val Acc: 64.98%: 100%|██████████| 196/196 [00:01<00:00, 184.79it/s]\n",
            "epoch 27 - train Loss: 6.607e+01 - train Acc: 72.00%: 100%|██████████| 586/586 [00:05<00:00, 115.53it/s]\n",
            "epoch 27 - val Loss: 6.551e-01 - val Acc: 64.43%: 100%|██████████| 196/196 [00:01<00:00, 182.41it/s]\n",
            "epoch 28 - train Loss: 6.630e+01 - train Acc: 71.59%: 100%|██████████| 586/586 [00:05<00:00, 116.03it/s]\n",
            "epoch 28 - val Loss: 6.342e-01 - val Acc: 65.20%: 100%|██████████| 196/196 [00:01<00:00, 185.85it/s]\n",
            "epoch 29 - train Loss: 6.629e+01 - train Acc: 71.69%: 100%|██████████| 586/586 [00:05<00:00, 115.41it/s]\n",
            "epoch 29 - val Loss: 6.408e-01 - val Acc: 64.94%: 100%|██████████| 196/196 [00:01<00:00, 186.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "xt = torch.Tensor(X_train)\n",
        "xt = xt.to(device)\n",
        "\n",
        "t = model(xt)"
      ],
      "metadata": {
        "id": "mTqCmBVkG2bd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = torch.max(t.data,1)[1]"
      ],
      "metadata": {
        "id": "aH8IFEU2IFjT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "# confusion_matrix(predicted.cpu(), y_test)\n",
        "f1_score(predicted.cpu(), y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSrFCrWhHsIz",
        "outputId": "f21e09c8-9064-417c-8a81-d0439433649a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40761224797626827"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"/content/models/model\")"
      ],
      "metadata": {
        "id": "VsO9e1U0i_um"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}